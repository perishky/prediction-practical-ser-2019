---
title: "Crash course introduction to prediction computation"
date: "`r format(Sys.time(), '%B %Y')`"
author: "Paul Yousefi, PhD, MPH"
output:
  html_document:
    css: style.css
    highlight: tango
    toc: true
  pdf_document: default
---

# Before we start

This lab will performed in R and will use the following packages:

* `pROC`
* `caret`
* `glmnet`
* `ranger`
* `kernlab`


We'll be using data from Tsaprouni et al. 2014 that is publicly available on the NCBI Gene Expression Ombibus (GEO) website (accession number: GSE50660) at https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE50660

All course material, including the data and code used for this lab practical, is available for you to download here:

__url for where to download data__


# Goals 

* Partitioning data into training and testing sets
* Evaluating performance of risk scores
* Fitting models in training data
* Predicting outputs from those models in the testing data
* Quantifying model prediction performance


```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(
comment = ">",
error = FALSE,
tidy = FALSE,
echo = TRUE, 
warning = FALSE, 
message = FALSE, 
cache=F)
```
```{r call_source, echo = F}
path <- "."
read_chunk(file.path(path, "prediction-practical-2019-ser-sourcecode.r"))
```
```{r globals, echo = F, results="hide", message = F, warning = F}
```


# Getting started 

To start, I'll load our data into active memory and have a look at what's available:

```{r load, echo = T} 
```
```{r ls, eval = F, echo=T} 
```
> [1] "meth"    "samples"


So we have two data objects:

* `meth` with DNA methylation data

* `samples` with other phenotype information on the participants of this study


Let's get a better sense of the variables available in `samples`:
 
```{r qc1, eval = T, echo=T} 
```

The `smoking` variable has 3 categories, but it's easiest to begin with a binary outcome so let's focus on the `ever.smoke` variable that collapses the __current__ and __former__ subjects into a single category

* When I talk about predicting smoking going from now on I'll be referring to this `ever.smoke`variable 

# Applying risk scores

### Single variable scores

The simplest type of risk score we can use for prediction is just a single individual variable. The site `cg05575921` in the _AHRR_ gene has consistently been the CpG with methylation showing the strongest association with smoking in several studies looking broadly across the genome.

Perhaps the methylation levels of this site would be sufficient to predict whether someone has been a smoker. To see, let's begin by adding this CpG site as a variable to our phenotype data object `samples`:

```{r ahrr} 
```

We can use a package called `pROC` to see how well different values of our `ahrr` variable explain smoking status:

```{r roc1} 
```

We can also visualize our results by using the `pROC` package's `plot.roc()` function on the saved output

```{r plot.roc}
```

### Weighted risk scores from published coefficients

Another common approach in omic prediction is to apply a risk score using information from multiple loci or omic measures weighted by previously reported magnitudes of association observed between those features and the outcome of interest. This has perhaps been most often performed using genetic data to compose 'polygenic risk scores', but can easily be extended to other types of data input.

For example, in the context of DNA methylation data we can define and subsequently apply a smoking score derived from the published coefficients of the largest blood epigenome wide association study (EWAS) meta-analysis to date in Joehanes et al. 2016:

>Joehanes R, Just AC, Marioni RE, Pilling LC, Reynolds LM, Mandaviya PR et al. Epigenetic Signatures of Cigarette Smoking. Circ Cardiovasc Genet 2016;

* The coefficients from their models were distributed in their Supplemental Table 2 that I previously saved and load into R below: 

```{r load.joehanes} 
```

The `joehanes` object has summary information on the `r nrow(joehanes)` CpGs that were significant at a Bonferroni p-value threshold in the original meta-analysis and that were available in our methylation dataset

```{r joehanes_str} 
```

Let's restrict our big methylation data object, `meth`, to just the CpGs that are in the `joehanes` list. This keeps the CpGs that we expect to be most related to smoking behavior, in the correct format, while reducing the size of the data were working with:

```{r subset.meth} 
```

To compute an individual's risk score using these coefficients, we need to take the sum of each coefficient multiplied by the participant's value at the corresponding variable, for example:

$$\hat{Y}_{Joehanes Score} = \sum_{i}^{2617 CpGs} \hat{\beta}_{i} X_{i}$$

* Where $\hat{\beta}_{i}$ are the individual previously estimated Joehanes et al. coefficients and $X_i$ are their corresponding variables (CpG site measurements in this case).

Equivalently, we can compute the exact same quantity, $\hat{Y}_{Joehanes Score}$, more simply using matrix multiplication:

$$\hat{Y}_{Joehanes Score} = X \hat{\beta}$$

* Where X is an $N x P$ matrix of all P variables being used and $\hat{\beta}$ is a corresponding vector of all Joehanes et al. coefficients.


To implement this, lets start by making a named vector of the `joehanes` coefficients:

```{r make_coefs} 
```

We can then use matrix multiplication against our observed DNA methylation values to get our $\hat{Y}_{Joehanes Score}$ values:

```{r apply_coefs} 
```

By adding this output as a variable to our `samples` data, we can again use the `pROC` package to evaluate and visualize the prediction performance of this score:


```{r add_yhat}
```

```{r plot.roc.again}
```

Does the joehanes score predict never/ever smoking better than just cg05575921 alone?

```{r comp_roc}
```

No, it doesn't appear to! Can you think of any possible explanations for why this might be the case??


# Training a novel predictor

Up until now, we've been evaluating the performance of predictors that other independent studies had previously developed. To develop our own prediction models, we first need to split our dataset into two parts:

1. a training set: the major subset of data where we can _fit_ our prediction models and estimate their relevant parameter values with access to the outcome variable observations

2. testing set: a smaller sub-set of observations withheld from training where we can _apply_ the models with trained and get an independent assessment of the predictor performance

This data splitting approach is essentially replicating within our own dataset the design we used implicitly to evaluate the performance of Joehanes et al. 2016 smoking score:

* There, the observations in the original Joehanes et al. 2016 paper served as the training set and the Tibshirani et al. data we've been using was the testing set

* Here, without access to two independent datasets we'll need to artificially generate a partition within our single dataset: reserving a subset of observations for training, and the remaining subset for testing


The `caret` package has functions to help with splitting the observations of your dataset along with a suite of other tools commonly used to train and test prediction models.

One of these is the `createDataPartition()` function which creates an index of the rows that will be allocated for training vs. testing at a user-selected percentage split of the observations. Below we dedicate 75% percent of our data for training and reserve 25% for testing:

```{r data.partition}
```

The index returned by `createDataPartition()` can then be easily used to split our `samples` data object into two separate objects:

```{r data.subset}
```

### Manual model training

Once these two independent set of observations have been generated, we can easily fit any model we can think up in the training subset and assess how well it performs in the reserved testing subset.

For example, if we wanted to see how well a linear subset of the `r nrow(joehanes)` CpGs that were bonferroni significant in Joehanes et al. 2016 performed at predicting our `ever.smoke` variable we could fit a penalized regression model like a lasso (least absolute shrinkage and selection operator) in our training dataset:

```{r lasso}
```

And then predict from that model into the reserved testing observations:

```{r pred.lasso}
```

All of the tools we'd employed previously from the `pROC` package can still be used to asses how well our new predictions performed:

```{r roc.lasso}
```

Up until now, the output we'd been returning from the `predict()` function had all been on 0 to 1 probability scale (i.e. the probability the model predict of an observation being a 'ever smoker') because we'd used the `type = "response"` argument.

However, `predict()` can alternately return the class that the model predicts is most probable by changing the `type` argument to be `type = "class"`. 

```{r confusion.pred.lasso}
```

This is useful to be able to use an additional function called `confusionMatrix()` in the `caret` package that allows us to more carefully compare how our predictions correspond to the truth and what errors we're experiencing 

```{r confusion.lasso}
```

### Caret model training

While any model can conceivably be trained/tested in our partitioned datasets, we need to know the R-package that distributes that model and the potentially idiosyncratic syntax for fitting that particular model. 

However, the `caret` package's function `train()` is a wrapper the conveniently facilitates a common syntax for implementing many common machine learning/prediction models. This makes it easier to quickly deploy a diverse set of candidate models with less hassle required to develop model/package-specific code.

For example, we can perform a rather basic call to `train()` in order to fit a RandomForest model from the `ranger` package: 

```{r load.fits, echo = F}
```

```{r rf, eval = F}
```

Fitting our training model with this approach still allows us to predict and evaluate model fits in the testing dataset in the ways we've learned so far:

```{r pred.rf}
```

Conveniently however, we can train a host of different models by only modifying the `method` argument in `train()`. Thus, deciding we would additionally like to fit a Support Vector Machines model from the `kernlab` package only requires updating our method to `method = "svmRadial"`

```{r svm, eval = F}
```

```{r pred.svm}
```

An extremely diverse selection of models is available through `train()` and details on the implementation of each is listed on the `caret` website http://topepo.github.io/caret/train-models-by-tag.html


```{r caret.models}
```

It should additionally be noted that the `train()` function has powerful functionality beyond simply providing convenient code for calling a large variety of different models. However, these will be explored in great depth later in the workshop. 

Be patient young padawan!


![](gfx/yoda.jpeg "Jedi prediction is powerful!")

